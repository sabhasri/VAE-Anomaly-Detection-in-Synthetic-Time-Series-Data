{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabhasri/VAE-Anomaly-Detection-in-Synthetic-Time-Series-Data/blob/main/VAE_Anomaly_Detection_in_Synthetic_Time_Series_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "num_dims = 4\n",
        "total_timesteps = 10000\n",
        "window_size = 50\n",
        "stride = 5\n",
        "anomaly_ratio = 0.02\n",
        "latent_dim = 8\n",
        "hidden_dim = 256\n",
        "batch_size = 64\n",
        "lr = 1e-3\n",
        "num_epochs = 30\n",
        "beta = 4.0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint_path = \"vae_timeseries_model.pth\"\n",
        "\n",
        "\n",
        "t = np.arange(total_timesteps)\n",
        "series = np.zeros((total_timesteps, num_dims), dtype=float)\n",
        "\n",
        "regimes = [\n",
        "    {\"start\": 0, \"end\": 3000, \"freq\": 0.02, \"amp\": 1.0, \"noise\": 0.1},\n",
        "    {\"start\": 3000, \"end\": 6000, \"freq\": 0.05, \"amp\": 0.6, \"noise\": 0.08},\n",
        "    {\"start\": 6000, \"end\": 8000, \"freq\": 0.01, \"amp\": 1.5, \"noise\": 0.12},\n",
        "    {\"start\": 8000, \"end\": 10000, \"freq\": 0.03, \"amp\": 0.9, \"noise\": 0.07},\n",
        "]\n",
        "\n",
        "for reg in regimes:\n",
        "    s, e = reg[\"start\"], reg[\"end\"]\n",
        "    tt = np.arange(e - s)\n",
        "    for d in range(num_dims):\n",
        "        phase = np.random.randn() * 2 * math.pi\n",
        "        series[s:e, d] = reg[\"amp\"] * np.sin(2 * math.pi * reg[\"freq\"] * tt + phase) + \\\n",
        "                         0.2 * d * np.cos(2 * math.pi * (reg[\"freq\"] * 0.5) * tt + phase/2)\n",
        "    series[s:e, :] += np.random.normal(scale=reg[\"noise\"], size=(e - s, num_dims))\n",
        "\n",
        "mean = series.mean(axis=0)\n",
        "std = series.std(axis=0)\n",
        "series = (series - mean) / (std + 1e-9)\n",
        "\n",
        "windows = []\n",
        "starts = list(range(0, total_timesteps - window_size + 1, stride))\n",
        "for s in starts:\n",
        "    windows.append(series[s:s + window_size])\n",
        "windows = np.stack(windows, axis=0)\n",
        "num_windows = windows.shape[0]\n",
        "print(f\"Generated {num_windows} windows (window={window_size}, stride={stride})\")\n",
        "\n",
        "num_anomalies = max(1, int(math.ceil(anomaly_ratio * num_windows)))\n",
        "anom_idx = set(np.random.choice(num_windows, size=num_anomalies, replace=False))\n",
        "labels = np.zeros(num_windows, dtype=int)\n",
        "\n",
        "for idx in anom_idx:\n",
        "    labels[idx] = 1\n",
        "    kind = np.random.choice([\"shift\", \"spike\", \"variance\"])\n",
        "    if kind == \"shift\":\n",
        "        offset = np.random.uniform(3.0, 6.0) * (np.random.choice([-1, 1]))\n",
        "        dims = np.random.choice(num_dims, size=max(1, num_dims//2), replace=False)\n",
        "        windows[idx, :, dims] += offset\n",
        "    elif kind == \"spike\":\n",
        "        for _ in range(np.random.randint(1, 4)):\n",
        "            tpos = np.random.randint(0, window_size)\n",
        "            dpos = np.random.randint(0, num_dims)\n",
        "            windows[idx, tpos, dpos] += np.random.uniform(5.0, 10.0) * (np.random.choice([-1, 1]))\n",
        "    else:\n",
        "        windows[idx] += np.random.normal(scale=2.5, size=(window_size, num_dims))\n",
        "\n",
        "print(f\"Injected {num_anomalies} anomalies (~{anomaly_ratio*100:.2f}%)\")\n",
        "\n",
        "\n",
        "class TimeSeriesWindowDataset(Dataset):\n",
        "    def __init__(self, windows, labels):\n",
        "        self.windows = windows.astype(np.float32)\n",
        "        self.labels = labels.astype(np.int64)\n",
        "    def __len__(self):\n",
        "        return len(self.windows)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.windows[idx], self.labels[idx]\n",
        "\n",
        "dataset = TimeSeriesWindowDataset(windows, labels)\n",
        "\n",
        "normal_idx = np.where(labels == 0)[0]\n",
        "anomaly_idx = np.where(labels == 1)[0]\n",
        "np.random.shuffle(normal_idx)\n",
        "\n",
        "n_train_norm = int(0.8 * len(normal_idx))\n",
        "n_val_norm = int(0.1 * len(normal_idx))\n",
        "train_idx = normal_idx[:n_train_norm]\n",
        "val_idx = np.concatenate([normal_idx[n_train_norm:n_train_norm + n_val_norm], anomaly_idx[:len(anomaly_idx)//2]])\n",
        "test_idx = np.concatenate([normal_idx[n_train_norm + n_val_norm:], anomaly_idx[len(anomaly_idx)//2:]])\n",
        "\n",
        "train_loader = DataLoader(torch.utils.data.Subset(dataset, train_idx),\n",
        "                          batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(torch.utils.data.Subset(dataset, val_idx), batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(torch.utils.data.Subset(dataset, test_idx), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train normals: {len(train_idx)}, Val samples: {len(val_idx)}, Test samples: {len(test_idx)}\")\n",
        "\n",
        "\n",
        "input_dim = window_size * num_dims\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(hidden_dim // 2, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim // 2, latent_dim)\n",
        "\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x_flat = x.view(B, -1)\n",
        "        h = self.enc(x_flat)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon_flat = self.dec(z)\n",
        "        recon = recon_flat.view(B, window_size, num_dims)\n",
        "        return recon, mu, logvar\n",
        "\n",
        "model = VAE(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=latent_dim).to(device)\n",
        "\n",
        "\n",
        "def elbo_loss(recon_x, x, mu, logvar, beta=1.0):\n",
        "    recon_loss = ((recon_x - x) ** 2).view(x.size(0), -1).sum(dim=1)\n",
        "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
        "    loss = recon_loss + beta * kl\n",
        "    return loss.mean(), recon_loss.detach().cpu().numpy(), kl.detach().cpu().numpy()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Training starting...\")\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for bx, _ in train_loader:\n",
        "        bx = bx.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon, mu, logvar = model(bx)\n",
        "        loss, _, _ = elbo_loss(recon, bx, mu, logvar, beta=beta)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * bx.size(0)\n",
        "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch}/{num_epochs} - train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "all_recon_scores = []\n",
        "all_elbo_scores = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for bx, by in test_loader:\n",
        "        bx = bx.to(device)\n",
        "        recon, mu, logvar = model(bx)\n",
        "        recon_err = ((recon - bx) ** 2).view(bx.size(0), -1).sum(dim=1).cpu().numpy()\n",
        "        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).cpu().numpy()\n",
        "        elbo = recon_err + beta * kl\n",
        "        all_recon_scores.extend(recon_err.tolist())\n",
        "        all_elbo_scores.extend(elbo.tolist())\n",
        "        all_labels.extend(by.numpy().tolist())\n",
        "\n",
        "all_recon_scores = np.array(all_recon_scores)\n",
        "all_elbo_scores = np.array(all_elbo_scores)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "def compute_pr_and_best(scores, labels):\n",
        "    prec, rec, thr = precision_recall_curve(labels, scores)\n",
        "    pr_auc = auc(rec, prec)\n",
        "\n",
        "    f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
        "    best_idx = np.nanargmax(f1)\n",
        "\n",
        "    if best_idx == 0:\n",
        "        best_thresh = thr[0] if len(thr)>0 else None\n",
        "    else:\n",
        "        best_thresh = thr[best_idx - 1]\n",
        "    return pr_auc, best_thresh, f1[best_idx], prec, rec, thr\n",
        "\n",
        "pr_recon, thresh_recon, bestf1_recon, prec_recon, rec_recon, thr_recon = compute_pr_and_best(all_recon_scores, all_labels)\n",
        "pr_elbo, thresh_elbo, bestf1_elbo, prec_elbo, rec_elbo, thr_elbo = compute_pr_and_best(all_elbo_scores, all_labels)\n",
        "\n",
        "def recall_at_precision(precision, recall, thresholds, target_p=0.95):\n",
        "    inds = np.where(precision >= target_p)[0]\n",
        "    if len(inds) == 0:\n",
        "        return 0.0, None\n",
        "    best_ind = inds[np.argmax(recall[inds])]\n",
        "    if best_ind == 0:\n",
        "        thr = thresholds[0] if len(thresholds)>0 else None\n",
        "    else:\n",
        "        thr = thresholds[best_ind - 1]\n",
        "    return recall[best_ind], thr\n",
        "\n",
        "rec95_recon, thr95_recon = recall_at_precision(prec_recon, rec_recon, thr_recon, target_p=0.95)\n",
        "rec95_elbo, thr95_elbo = recall_at_precision(prec_elbo, rec_elbo, thr_elbo, target_p=0.95)\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    \"Method\": [\"Reconstruction Error\", \"ELBO (recon + beta*KL)\"],\n",
        "    \"AUC-PR\": [pr_recon, pr_elbo],\n",
        "    \"Best F1\": [bestf1_recon, bestf1_elbo],\n",
        "    \"Recall@95%Prec\": [rec95_recon, rec95_elbo],\n",
        "    \"Best Threshold\": [thresh_recon, thresh_elbo]\n",
        "})\n",
        "print(\"\\nResults:\\n\", results.to_string(index=False))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(rec_recon, prec_recon, label=\"Recon error PR\")\n",
        "plt.plot(rec_elbo, prec_elbo, label=\"ELBO PR\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall curves\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"mean\": mean,\n",
        "    \"std\": std,\n",
        "    \"window_size\": window_size,\n",
        "    \"num_dims\": num_dims,\n",
        "    \"beta\": beta\n",
        "}, checkpoint_path)\n",
        "print(f\"Saved model checkpoint to {checkpoint_path}\")\n"
      ],
      "metadata": {
        "id": "YYQVBb_aeMNy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
